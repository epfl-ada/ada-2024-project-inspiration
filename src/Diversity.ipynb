{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diversity: \n",
    "1. Reorganise ethnicities into a few groups\n",
    "2. Establish the Naive Coefficient\n",
    "3. Add the multiplication by extra coefficient rewarding equal representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. Start by making the character_df\n",
    "NB: final name should be \"filtered_character\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the dataframe is:(450668, 13)\n",
      "Index(['975900', '/m/03vyhn', '2001-08-24', 'Akooshay', '1958-08-26', 'F',\n",
      "       '1.62', 'Unnamed: 7', 'Wanda De Jesus', '42', '/m/0bgchxw',\n",
      "       '/m/0bgcj3x', '/m/03wcfv7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "character_metadata = pd.read_csv('../data/character.metadata.tsv', sep='\\t')\n",
    "\n",
    "# Look through the DataFrames:\n",
    "print(f'the size of the dataframe is:{character_metadata.shape}') #-- (450668, 13)\n",
    "print(character_metadata.columns) #need to rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "new_column_names = [\n",
    "    \"Wikipedia_movie_ID\",\n",
    "    \"Freebase_movie_ID\",\n",
    "    \"Movie_release_date\",\n",
    "    \"Character_name\",\n",
    "    \"Actor_date_of_birth\",\n",
    "    \"Actor_gender\",\n",
    "    \"Actor_height_m\",\n",
    "    \"Actor_ethnicity\",\n",
    "    \"Actor_name\",\n",
    "    \"Actor_age_at_movie_release\",\n",
    "    \"Freebase_character_actor_map_ID\",\n",
    "    \"Freebase_character_ID\",\n",
    "    \"Freebase_actor_ID\"\n",
    "]\n",
    "character_metadata.columns = new_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows for 'Wikipedia_movie_ID': 450668\n",
      "Remaining rows for 'Movie_release_date': 440673\n",
      "Remaining rows for 'Actor_ethnicity': 106058\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = ['Wikipedia_movie_ID', 'Movie_release_date', 'Actor_ethnicity']\n",
    "remaining_rows = {col: character_metadata[col].dropna().shape[0] for col in columns_to_check}\n",
    "for col, count in remaining_rows.items():\n",
    "    print(f\"Remaining rows for '{col}': {count}\")\n",
    "\n",
    "filtered_character = character_metadata[['Wikipedia_movie_ID', 'Movie_release_date', 'Actor_ethnicity']].dropna(subset=['Actor_ethnicity']) # no missing value remaining in each col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#homogeneous release dates (only the year)\n",
    "filtered_character['Movie_release_date'] = filtered_character['Movie_release_date'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe, but with freebase_id for ethnicity. We define the function to find the right labels corresponding to the id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_to_label(freebase_id,conversion_table):\n",
    "    if freebase_id in conversion_table.index:\n",
    "        return conversion_table.loc[freebase_id,'label']\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found a table online with freebase_id, wikidata_id & Label\n",
    "We import it, but it has 2 million lines --> we will select only the lines we need, i.e. the lines of the ethnicity id we have in our filtered_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the file, setting freebase_id as index allows us to use .loc later\n",
    "fb_wiki_gen = pd.read_csv('../data/fb_wiki_mapping.tsv',sep='\\t')\n",
    "fb_wiki_gen.set_index('freebase_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of different existing ethnicities\n",
    "ethnicities_list = filtered_character['Actor_ethnicity'].unique().tolist()\n",
    "#now select those from the fb_wiki_gen\n",
    "fb_wiki_ethnic = fb_wiki_gen.loc[fb_wiki_gen.index.isin(ethnicities_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now change the Actor_ethnicity column from freebase_id to label\n",
    "filtered_character['Actor_ethnicity']=filtered_character['Actor_ethnicity'].apply(fb_to_label,conversion_table=fb_wiki_ethnic)\n",
    "#some freebase_ids couldn't be found, so we get None. We will now drop those None values\n",
    "filtered_character = filtered_character.dropna(subset=['Actor_ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Movie_release_date</th>\n",
       "      <th>Actor_ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212076</th>\n",
       "      <td>9105282</td>\n",
       "      <td>1997</td>\n",
       "      <td>Argentines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167859</th>\n",
       "      <td>8499221</td>\n",
       "      <td>2010</td>\n",
       "      <td>English people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190110</th>\n",
       "      <td>33747977</td>\n",
       "      <td>1948</td>\n",
       "      <td>Portuguese Americans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376856</th>\n",
       "      <td>11120865</td>\n",
       "      <td>1993</td>\n",
       "      <td>Malaysian Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328648</th>\n",
       "      <td>6739637</td>\n",
       "      <td>2007</td>\n",
       "      <td>Indian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Wikipedia_movie_ID Movie_release_date       Actor_ethnicity\n",
       "212076             9105282               1997            Argentines\n",
       "167859             8499221               2010        English people\n",
       "190110            33747977               1948  Portuguese Americans\n",
       "376856            11120865               1993     Malaysian Chinese\n",
       "328648             6739637               2007                Indian"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_character.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the right dataframe to Analyse diversity, but we have too many different ethnicities. We want to remove the very specific ones and group them together in more general ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''\n",
    "\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_character['Actor_ethnicity']=filtered_character['Actor_ethnicity'].astype(str)\n",
    "ethnicities_labels = filtered_character['Actor_ethnicity'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['African Americans',\n",
       " 'Omaha people',\n",
       " 'Jewish people',\n",
       " 'Irish Americans',\n",
       " 'Indian Americans',\n",
       " 'Italians',\n",
       " 'German Americans',\n",
       " 'Indian',\n",
       " 'Ezhava',\n",
       " 'Malayali',\n",
       " 'Taiwanese',\n",
       " 'Armenians',\n",
       " 'Marathi people',\n",
       " 'Lithuanian American',\n",
       " 'Italian Americans',\n",
       " 'Danish Americans',\n",
       " 'American Jews',\n",
       " 'Scottish Americans',\n",
       " 'Puerto Ricans',\n",
       " 'English people',\n",
       " 'Irish people',\n",
       " 'Russian Americans',\n",
       " 'English Americans',\n",
       " 'Gujarati people',\n",
       " 'Spanish Americans',\n",
       " 'Bihari people',\n",
       " 'Nair',\n",
       " 'Cuban Americans',\n",
       " 'Russians',\n",
       " 'Yoruba people',\n",
       " 'Japanese people',\n",
       " 'Filipino Americans',\n",
       " 'Swedish Americans',\n",
       " 'Finnish Americans',\n",
       " 'Koreans',\n",
       " 'French',\n",
       " 'Welsh people',\n",
       " 'White Americans',\n",
       " 'Bengali',\n",
       " 'Uruguayans',\n",
       " 'Iranian peoples',\n",
       " 'Mexicans',\n",
       " 'Dutch Americans',\n",
       " 'Hungarian Americans',\n",
       " 'Spaniards',\n",
       " 'Bunt',\n",
       " 'Swedes',\n",
       " 'Sindhis',\n",
       " 'Tamil',\n",
       " 'Italian Canadians',\n",
       " 'Asian Americans',\n",
       " 'Mexican Americans',\n",
       " 'Punjabis',\n",
       " 'White British',\n",
       " 'Scottish Australian',\n",
       " 'White Africans of European ancestry',\n",
       " 'Americans',\n",
       " 'Germans',\n",
       " 'Stateside Puerto Ricans',\n",
       " 'Cherokee',\n",
       " 'Serbian Americans',\n",
       " 'Dominican Americans',\n",
       " 'Polish Americans',\n",
       " 'Austrians',\n",
       " 'Hongkongers',\n",
       " 'French Canadians',\n",
       " 'Muslim',\n",
       " 'Pashtuns',\n",
       " 'British Indian',\n",
       " 'Telugu people',\n",
       " 'Kayastha',\n",
       " 'Irish migration to Great Britain',\n",
       " 'Filipino people',\n",
       " 'Sri Lankan Tamils',\n",
       " 'names of the Greeks',\n",
       " 'Brahmin caste',\n",
       " 'Italian Australian',\n",
       " 'Australian Americans',\n",
       " 'Czech Americans',\n",
       " 'Kashmiri Pandit',\n",
       " 'Jaat',\n",
       " 'Serbs of Croatia',\n",
       " 'British Americans',\n",
       " 'Norwegians',\n",
       " 'Bosnians',\n",
       " 'Sicilian Americans',\n",
       " 'English Australian',\n",
       " 'Australians',\n",
       " 'Danes',\n",
       " 'French Chilean',\n",
       " 'Albanian American',\n",
       " 'Austrian Americans',\n",
       " 'Romani people',\n",
       " 'Greeks in South Africa',\n",
       " 'French Americans',\n",
       " 'Native Hawaiians',\n",
       " 'Slovaks',\n",
       " 'Chinese Americans',\n",
       " 'Greek Americans',\n",
       " 'Hungarians',\n",
       " 'Romanichal',\n",
       " 'Parsi',\n",
       " 'Indonesian Americans',\n",
       " 'Colombian Americans',\n",
       " 'Slovak Americans',\n",
       " 'Lebanese Americans',\n",
       " 'Anglo-Celtic Australians',\n",
       " 'Swiss',\n",
       " 'Que viva el amor de Chaves',\n",
       " 'Latvians',\n",
       " 'Norwegian Americans',\n",
       " 'Belgians',\n",
       " 'Portuguese Americans',\n",
       " 'Bolivian American',\n",
       " 'Kiwi',\n",
       " 'Konkani people',\n",
       " 'Cubans',\n",
       " 'Armenian American',\n",
       " 'Irish Canadians',\n",
       " 'Kannada people',\n",
       " 'Italian Brazilians',\n",
       " 'Taiwanese Americans',\n",
       " 'English Canadians',\n",
       " 'Cajun',\n",
       " 'Pakistanis',\n",
       " 'Syrian Americans',\n",
       " 'Pacific Islander Americans',\n",
       " 'Chilean American',\n",
       " 'Israelis',\n",
       " 'Afro-Cuban',\n",
       " 'Dutch',\n",
       " 'White South Africans',\n",
       " 'Welsh Americans',\n",
       " 'Nepali Indian',\n",
       " 'Slavs',\n",
       " 'First Nations',\n",
       " 'British Pakistanis',\n",
       " 'Iranians in the United Kingdom',\n",
       " 'Akan people',\n",
       " 'Croatian Canadians',\n",
       " 'Māori',\n",
       " 'Black Britons',\n",
       " 'Chinese Filipino',\n",
       " 'Dalmatian Italians',\n",
       " 'Ashkenazi Jews',\n",
       " 'Romanian Americans',\n",
       " 'freebase_id\\n/m/017sq0    Eurasian\\n/m/017sq0     Eurasia\\nName: label, dtype: object',\n",
       " 'Chinese Singaporeans',\n",
       " 'Korean American',\n",
       " 'History of the Jews in Morocco',\n",
       " 'British Jews',\n",
       " 'Lebanese',\n",
       " 'Catalan people',\n",
       " 'Sikh',\n",
       " 'Belarusians',\n",
       " 'Niyogi',\n",
       " 'Samoan New Zealanders',\n",
       " 'Blackfoot Confederacy',\n",
       " 'British Chinese',\n",
       " 'Polish Canadians',\n",
       " 'Sámi peoples',\n",
       " 'Soviet people',\n",
       " 'Filipino Australians',\n",
       " 'Czechs',\n",
       " 'Argentines',\n",
       " 'Anglo-Indian people',\n",
       " 'Croatian Americans',\n",
       " 'Hispanic',\n",
       " 'Tulu people',\n",
       " 'Serbs in the United Kingdom',\n",
       " 'Kapampangan people',\n",
       " 'Ilocano',\n",
       " 'Ojibwe',\n",
       " 'Marwari',\n",
       " 'Brazilian Americans',\n",
       " 'German Canadians',\n",
       " 'Castilians',\n",
       " 'Bhutia',\n",
       " 'Iraqi Americans',\n",
       " 'Bosniaks',\n",
       " 'Filipino people of Spanish ancestry',\n",
       " 'Sephardi Jews',\n",
       " 'Brazilians',\n",
       " 'Berber',\n",
       " 'Serbian Canadians',\n",
       " 'Israeli Americans',\n",
       " 'Greek Canadians',\n",
       " 'Moroccans',\n",
       " 'history of the Jews in India',\n",
       " 'Kikuyu',\n",
       " 'Xhosa people',\n",
       " 'Egyptians',\n",
       " 'Poles',\n",
       " 'Dutch Australian',\n",
       " 'Serbs in North Macedonia',\n",
       " 'Arab Americans',\n",
       " 'Croats',\n",
       " 'Thai Chinese people',\n",
       " 'Galicians',\n",
       " 'Palestinian Americans',\n",
       " 'Greek Cypriots',\n",
       " 'Poles in the United Kingdom',\n",
       " 'Ecuadorian Americans',\n",
       " 'Mohawk people',\n",
       " 'Inuit',\n",
       " 'Sioux',\n",
       " 'Lumbee',\n",
       " 'Malaysian Chinese',\n",
       " 'Anglo-Irish people',\n",
       " 'Slovene Americans',\n",
       " 'Aromanians',\n",
       " 'Swedish-speaking population of Finland',\n",
       " 'Bulgarian Canadians',\n",
       " 'Hmong American',\n",
       " 'Ukrainian Americans',\n",
       " 'Arabs in Bulgaria',\n",
       " 'British Asian',\n",
       " 'Koryo-saram',\n",
       " 'White Latin American',\n",
       " 'Italians in the United Kingdom',\n",
       " 'Finns',\n",
       " 'Irish Australian',\n",
       " 'Lao people',\n",
       " 'Croatian Australians',\n",
       " 'Vietnamese people',\n",
       " 'Kashmiri people',\n",
       " 'Rohilla',\n",
       " 'Hondurans',\n",
       " 'Indian diaspora in France',\n",
       " 'Chinese Indonesians',\n",
       " 'Danish Canadians',\n",
       " 'Ukrainians',\n",
       " 'Polish Australians',\n",
       " 'Agrawal',\n",
       " 'Albanians',\n",
       " 'Chinese Canadians',\n",
       " 'Javanese',\n",
       " 'Sudanese Arabs',\n",
       " 'Venezuelans',\n",
       " 'Arabs',\n",
       " 'Mudaliar',\n",
       " 'Sinhalese',\n",
       " 'Cree',\n",
       " 'Portuguese',\n",
       " 'Chinese Jamaicans',\n",
       " 'Acadians',\n",
       " 'Muhajir',\n",
       " 'Honduran Americans',\n",
       " 'Lebanese immigration to Mexico',\n",
       " 'Serbian Australians',\n",
       " 'Colombians',\n",
       " 'Hindu',\n",
       " 'Chileans',\n",
       " 'Indo-Canadians',\n",
       " 'Persians',\n",
       " 'African people',\n",
       " 'Bengali Brahmins',\n",
       " 'Swedish Canadians',\n",
       " 'Vietnamese Americans',\n",
       " 'Haitian Americans',\n",
       " 'Kabyle people',\n",
       " 'Bahamian Americans',\n",
       " 'Assyrian people',\n",
       " 'rajput',\n",
       " 'Turkish Americans',\n",
       " 'Chileans in the United Kingdom',\n",
       " 'Gibraltarian people',\n",
       " 'Icelanders',\n",
       " 'Copts',\n",
       " 'Choctaw',\n",
       " 'Louisiana Creole people',\n",
       " 'Lithuanian Jews',\n",
       " 'Salvadoran Americans',\n",
       " 'Venezuelan Americans',\n",
       " 'Khatri',\n",
       " 'Latino',\n",
       " 'Nigerian Americans',\n",
       " 'Apache',\n",
       " 'Yugoslavs',\n",
       " 'Bulgarians',\n",
       " 'Haudenosaunee',\n",
       " 'Somalis',\n",
       " 'Arab Mexican',\n",
       " 'Ryukyuan people',\n",
       " 'Bohemian',\n",
       " 'Ukrainian Canadians',\n",
       " 'Dutch Canadians',\n",
       " 'Luxembourgish Americans',\n",
       " 'Iranian Americans',\n",
       " 'Mandinka people',\n",
       " 'Tibetan people',\n",
       " 'Corsicans',\n",
       " 'Aceh',\n",
       " 'Sherpa',\n",
       " 'Romanians',\n",
       " 'Malagasy people',\n",
       " 'Filipino mestizo',\n",
       " 'Indian Australian',\n",
       " 'Serbs of Bosnia and Herzegovina',\n",
       " 'Manchu',\n",
       " 'Buryats',\n",
       " 'Cheyennes',\n",
       " 'Quebeckers',\n",
       " 'Armenians of Russia',\n",
       " 'Dene',\n",
       " 'multiracial people',\n",
       " 'Spanish immigration to Mexico',\n",
       " 'Romani people in Spain',\n",
       " 'Chettiar',\n",
       " 'Baltic Russians',\n",
       " 'Inupiat people',\n",
       " 'Transylvanian Saxons',\n",
       " 'Greek Australians',\n",
       " 'Latvian American',\n",
       " 'Panamanian Americans',\n",
       " 'Dalit',\n",
       " 'Afro-Asians',\n",
       " 'Basque people',\n",
       " 'British African-Caribbean people',\n",
       " 'Tatars',\n",
       " 'Tejano',\n",
       " 'Métis',\n",
       " 'Ossetians',\n",
       " 'Thai Americans',\n",
       " 'German Brazilians',\n",
       " 'Zhuang people',\n",
       " 'Italian immigration to Mexico',\n",
       " 'Manx people',\n",
       " 'Japanese Brazilians',\n",
       " 'Indian diaspora',\n",
       " 'Ho-Chunk',\n",
       " 'Greeks in the United Kingdom',\n",
       " 'Wolof people',\n",
       " 'culture of Palestine',\n",
       " 'Israeli Jews',\n",
       " 'Slovenes',\n",
       " 'Nez Perce',\n",
       " 'Swedish Australian',\n",
       " 'Dogra',\n",
       " 'Kurds',\n",
       " 'demographics of Iraq',\n",
       " 'Gin people',\n",
       " 'Hazaras',\n",
       " 'Hutsuls',\n",
       " 'Aymara',\n",
       " 'Oneida',\n",
       " 'Azerbaijanis',\n",
       " 'Thai people',\n",
       " 'Iranian Canadians',\n",
       " 'Afrikaners',\n",
       " 'peoples of the Caucasus',\n",
       " 'Sierra Leone Creole people',\n",
       " 'Georgians',\n",
       " 'Han Chinese people']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ethnicities_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethnic_spec_to_gen(ethnicities):\n",
    "    prompt = f\"Group the following ethnicities into broader categories: {ethnicities}\"\n",
    "    response = openai.Completion.create(\n",
    "        model = 'gpt-3.5-turbo',\n",
    "        prompt = prompt ,\n",
    "        max_tokens = 500,\n",
    "        temperature = 0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = {\n",
    "    'African': [\n",
    "        'African Americans', 'Yoruba people', 'Egyptians', 'Kikuyu', 'Xhosa people', 'Somalis', \n",
    "        'Mandinka people', 'Malagasy people', 'Afro-Cuban', 'Sudanese Arabs', 'Kabyle people', \n",
    "        'Nigerian Americans', 'Sierra Leone Creole people', 'Zulu', 'Berber', 'Blackfoot Confederacy','Mandinka', 'Kikuyu', 'Xhosa', 'Kabyle', 'Somalis', 'Berber', 'Afrikaners'\n",
    "    ],\n",
    "    'South Asian': [\n",
    "        'Indian', 'Ezhava', 'Malayali', 'Gujarati people', 'Bihari people', 'Punjabis', 'Pashtuns', \n",
    "        'Telugu people', 'Sri Lankan Tamils', 'Tamil', 'Kayastha', 'Nair', 'Bengali', 'Marwari', \n",
    "        'Sindhis', 'Punjabis', 'Rajput', 'Khatri', 'Bengali Brahmins', 'Kashmiri Pandit', 'Indian Americans','Marwari', 'Konkani', 'Kayastha', 'Niyogi', 'Tamil', 'Ezhava', 'Bengali Brahmins', \n",
    "        'Sindhis', 'Gujarati people', 'Punjabis', 'Sri Lankan Tamils', 'Telugu people'\n",
    "    ],\n",
    "    'Middle Eastern': [\n",
    "        'Jewish people', 'Israeli Americans', 'Palestinian Americans', 'Arabs', 'Persians', 'Kurdish', \n",
    "        'Tatars', 'Assyrian people', 'Azerbaijanis', 'Kurds', 'Lebanese Americans', 'Lebanese', \n",
    "        'Iranian Americans', 'Afghan', 'Turks', 'Armenians','Ashkenazi Jews', 'Sephardi Jews', 'Lebanese', 'Copts', 'Israelis', 'Arabs', 'Kurds', \n",
    "        'Tatars', 'Ossetians', 'Azerbaijanis', 'Persians', 'Iranians'\n",
    "    ],\n",
    "    'European or American': [\n",
    "        'Germans', 'Swedes', 'British Indian', 'Spaniards', 'British', 'Russians', 'French', \n",
    "        'Italians', 'Greek Americans', 'Finnish Americans', 'Scots', 'Irish Americans', \n",
    "        'White British', 'Irish migration to Great Britain', 'German Americans', 'Italians','Catalan people', 'Basque people', 'Latvians', 'Baltic Russians', 'Transylvanian Saxons',\n",
    "        'Corsicans', 'French Chilean', 'Italian Brazilians', 'Luxembourgish Americans', 'White South Africans',\n",
    "        'Portuguese Americans', 'French Americans', 'French Canadians', 'British Asian'\n",
    "    ],\n",
    "    'Indigenous': [\n",
    "        'Cherokee', 'Navajo', 'Sioux', 'Mohawk', 'Inuit', 'Metis', 'Quechua', 'Maya', 'Apache', \n",
    "        'Blackfoot Confederacy', 'Haudenosaunee', 'Ojibwe', 'Inupiat', 'Cheyenne', 'Taino', \n",
    "        'Comanche', 'Oneida', 'Zuni','Blackfoot', 'Mohawk', 'Inuit', 'Sioux', 'Lumbee', 'Cheyennes', 'Nez Perce', 'Oneida', \n",
    "        'Aymara', 'Inupiat people', 'Haudenosaunee', 'Apache', 'Ojibwe', 'Cherokee', 'Māori'\n",
    "    ],\n",
    "    'Latino': [\n",
    "        'Mexicans', 'Hispanic', 'Spaniards', 'Puerto Ricans', 'Uruguayans', 'Colombians', \n",
    "        'Brazilians', 'Argentines', 'Chilean Americans', 'Venezuelan Americans', 'Dominican Americans','Afro-Cuban', 'Chilean American', 'Mexican Americans', 'Spanish Americans', \n",
    "        'Uruguayans', 'Dominican Americans', 'Ecuadorian Americans', 'Colombians', \n",
    "        'Spanish immigration to Mexico', 'Venezuelans'\n",
    "    ],\n",
    "    'Pacific Islander': [\n",
    "        'Filipino Americans', 'Hawaiian', 'Samoans', 'Tongans', 'Maori', 'Fijians', \n",
    "        'Polynesian', 'Micronesian', 'Guamanian', 'Native Hawaiians', 'Marshallese','Filipino mestizo', 'Kapampangan', 'Samoan New Zealanders', 'Chinese Filipino',\n",
    "        'Vietnamese people', 'Ryukyuan people', 'Japanese Brazilians', 'Japanese Americans', \n",
    "        'Pacific Islander Americans'\n",
    "    ],\n",
    "    'Mixed': [\n",
    "        'Anglo-Indian people', 'Afro-Asians', 'Mulatto', 'Mestizo', 'Métis', 'Eurasian', \n",
    "        'British African-Caribbean', 'Hapa', 'Amerasians','Afro-Asians', 'multiracial people', 'Métis', 'British African-Caribbean people', 'White Latin American'\n",
    "    ],\n",
    "    'Other': [\n",
    "        'Han Chinese people', 'Japanese Brazilians', 'Dalit', 'Cossacks', 'Tatars', \n",
    "        'Romani people', 'Yakuts', 'Hazaras', 'Yugoslavs', 'Ashkenazi Jews', 'Catalan people', \n",
    "        'Corsicans', 'Serbs of Bosnia and Herzegovina', 'Aromanians','Koryo-saram', 'Buryats', 'Hmong American', 'Sierra Leone Creole people', 'Dene', \n",
    "        'Chettiar', 'Sherpa', 'Tibetan people', 'Malagasy people', 'Hazaras', 'Gin people', \n",
    "        'Aromanians', 'Romanichal'\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnicity_to_group = {}\n",
    "\n",
    "for group, ethnicities in groupings.items():\n",
    "    for ethnicity in ethnicities:\n",
    "        ethnicity_to_group[ethnicity] = group\n",
    "\n",
    "classified_ethnicities = {ethnicity: ethnicity_to_group.get(ethnicity, 'Unknown') for ethnicity in ethnicities_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Movie_release_date</th>\n",
       "      <th>Actor_ethnicity</th>\n",
       "      <th>ethnic_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31183</th>\n",
       "      <td>16983442</td>\n",
       "      <td>1999</td>\n",
       "      <td>Irish Americans</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440466</th>\n",
       "      <td>1362608</td>\n",
       "      <td>2006</td>\n",
       "      <td>English people</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414583</th>\n",
       "      <td>34495806</td>\n",
       "      <td>nan</td>\n",
       "      <td>Swedish Americans</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224052</th>\n",
       "      <td>10408933</td>\n",
       "      <td>1938</td>\n",
       "      <td>French Americans</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179381</th>\n",
       "      <td>358367</td>\n",
       "      <td>1977</td>\n",
       "      <td>Jewish people</td>\n",
       "      <td>Middle Eastern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78613</th>\n",
       "      <td>18785526</td>\n",
       "      <td>1950</td>\n",
       "      <td>White British</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220022</th>\n",
       "      <td>17987664</td>\n",
       "      <td>2008</td>\n",
       "      <td>Kapampangan people</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448978</th>\n",
       "      <td>967721</td>\n",
       "      <td>1987</td>\n",
       "      <td>African Americans</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130922</th>\n",
       "      <td>3731073</td>\n",
       "      <td>1957</td>\n",
       "      <td>Jewish people</td>\n",
       "      <td>Middle Eastern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341811</th>\n",
       "      <td>5721950</td>\n",
       "      <td>2003</td>\n",
       "      <td>Indian</td>\n",
       "      <td>South Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104423</th>\n",
       "      <td>18488743</td>\n",
       "      <td>2008</td>\n",
       "      <td>Indian</td>\n",
       "      <td>South Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257121</th>\n",
       "      <td>30627321</td>\n",
       "      <td>1987</td>\n",
       "      <td>Ukrainian Americans</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58905</th>\n",
       "      <td>28485791</td>\n",
       "      <td>2011</td>\n",
       "      <td>Stateside Puerto Ricans</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>12384049</td>\n",
       "      <td>1933</td>\n",
       "      <td>Irish Americans</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153187</th>\n",
       "      <td>29800838</td>\n",
       "      <td>2011</td>\n",
       "      <td>Indian</td>\n",
       "      <td>South Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291222</th>\n",
       "      <td>3572176</td>\n",
       "      <td>2006</td>\n",
       "      <td>Jewish people</td>\n",
       "      <td>Middle Eastern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6562</th>\n",
       "      <td>12576817</td>\n",
       "      <td>2008</td>\n",
       "      <td>Chinese Americans</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251421</th>\n",
       "      <td>15412011</td>\n",
       "      <td>1994</td>\n",
       "      <td>Indian</td>\n",
       "      <td>South Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150401</th>\n",
       "      <td>55495</td>\n",
       "      <td>1990</td>\n",
       "      <td>Poles</td>\n",
       "      <td>European or American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130856</th>\n",
       "      <td>34847933</td>\n",
       "      <td>1928</td>\n",
       "      <td>White Latin American</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Wikipedia_movie_ID Movie_release_date          Actor_ethnicity  \\\n",
       "31183             16983442               1999          Irish Americans   \n",
       "440466             1362608               2006           English people   \n",
       "414583            34495806                nan        Swedish Americans   \n",
       "224052            10408933               1938         French Americans   \n",
       "179381              358367               1977            Jewish people   \n",
       "78613             18785526               1950            White British   \n",
       "220022            17987664               2008       Kapampangan people   \n",
       "448978              967721               1987        African Americans   \n",
       "130922             3731073               1957            Jewish people   \n",
       "341811             5721950               2003                   Indian   \n",
       "104423            18488743               2008                   Indian   \n",
       "257121            30627321               1987      Ukrainian Americans   \n",
       "58905             28485791               2011  Stateside Puerto Ricans   \n",
       "1323              12384049               1933          Irish Americans   \n",
       "153187            29800838               2011                   Indian   \n",
       "291222             3572176               2006            Jewish people   \n",
       "6562              12576817               2008        Chinese Americans   \n",
       "251421            15412011               1994                   Indian   \n",
       "150401               55495               1990                    Poles   \n",
       "130856            34847933               1928     White Latin American   \n",
       "\n",
       "                ethnic_group  \n",
       "31183   European or American  \n",
       "440466  European or American  \n",
       "414583  European or American  \n",
       "224052  European or American  \n",
       "179381        Middle Eastern  \n",
       "78613   European or American  \n",
       "220022                   NaN  \n",
       "448978               African  \n",
       "130922        Middle Eastern  \n",
       "341811           South Asian  \n",
       "104423           South Asian  \n",
       "257121                   NaN  \n",
       "58905                    NaN  \n",
       "1323    European or American  \n",
       "153187           South Asian  \n",
       "291222        Middle Eastern  \n",
       "6562                     NaN  \n",
       "251421           South Asian  \n",
       "150401  European or American  \n",
       "130856                   NaN  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_character['ethnic_group'] = filtered_character['Actor_ethnicity'].map(ethnicity_to_group)\n",
    "filtered_character.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to create a coefficient for diversity.\n",
    "We start with a naive version, only counting the amount of different ethnicities and normalising over the number of actors mentioned for a film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_div = filtered_character.groupby('Wikipedia_movie_ID').agg(ethnicity_number=('ethnic_group','nunique'),actor_number=('Wikipedia_movie_ID','size')).reset_index()\n",
    "mov_div['naive_diversity']=mov_div['ethnicity_number']/mov_div['actor_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to add an entropy to reward equal representation.\n",
    "ethn_count = filtered_character.groupby(['Wikipedia_movie_ID','ethnic_group']).agg('size').reset_index(name='num_actors')\n",
    "tot_actors = ethn_count.groupby('Wikipedia_movie_ID')['num_actors'].transform('sum')\n",
    "\n",
    "ethn_count['proportion'] = ethn_count['num_actors']/tot_actors\n",
    "ethn_count['entropy'] = 1-ethn_count['proportion'] * np.log(ethn_count['proportion'])\n",
    "\n",
    "entropy_by_movie = ethn_count.groupby('Wikipedia_movie_ID')['entropy'].sum().reset_index()\n",
    "\n",
    "# Normalize entropy\n",
    "max_entropy = np.log(len(filtered_character['ethnic_group'].unique()))  # Maximum possible entropy\n",
    "entropy_by_movie['normalized_entropy'] = entropy_by_movie['entropy'] / max_entropy\n",
    "\n",
    "# merge everything back together\n",
    "diversity_final = mov_div.merge(entropy_by_movie[['Wikipedia_movie_ID', 'normalized_entropy']], on='Wikipedia_movie_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversity_final['diversity']= diversity_final['naive_diversity']*diversity_final['normalized_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>ethnicity_number</th>\n",
       "      <th>actor_number</th>\n",
       "      <th>naive_diversity</th>\n",
       "      <th>normalized_entropy</th>\n",
       "      <th>diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3217</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.217147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3333</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3746</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.169619</td>\n",
       "      <td>0.584809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3837</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.763608</td>\n",
       "      <td>0.587869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3947</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4227</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.169619</td>\n",
       "      <td>1.169619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4231</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.165172</td>\n",
       "      <td>0.291293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4560</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.062042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4726</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.112808</td>\n",
       "      <td>0.445123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4728</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.108574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4729</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.128414</td>\n",
       "      <td>0.250759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4730</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.086859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5035</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.217147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5224</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.217147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5313</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5729</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.086859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7906</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8481</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8695</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.679661</td>\n",
       "      <td>0.839831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8994</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9429</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9835</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.780005</td>\n",
       "      <td>1.780005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9979</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.327150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10193</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.217147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10977</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.763349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11223</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.112808</td>\n",
       "      <td>0.556404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11242</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.327150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11585</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11701</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.763349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12005</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>13149</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.572512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>13154</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13290</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13901</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.715581</td>\n",
       "      <td>1.029348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16782</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>17920</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.062042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>18996</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.327150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>19701</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.033407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>19715</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.112808</td>\n",
       "      <td>0.445123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20669</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20786</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.742131</td>\n",
       "      <td>0.871065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>21180</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.085911</td>\n",
       "      <td>0.434364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22216</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.715581</td>\n",
       "      <td>1.029348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>22751</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>22829</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.434294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23255</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434294</td>\n",
       "      <td>0.217147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>24653</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.145024</td>\n",
       "      <td>0.572512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>25491</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.085911</td>\n",
       "      <td>0.434364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Wikipedia_movie_ID  ethnicity_number  actor_number  naive_diversity  \\\n",
       "0                  330                 0             1         0.000000   \n",
       "1                 3217                 1             2         0.500000   \n",
       "2                 3333                 1             1         1.000000   \n",
       "3                 3746                 2             4         0.500000   \n",
       "4                 3837                 3             9         0.333333   \n",
       "5                 3947                 1             3         0.333333   \n",
       "6                 4227                 2             2         1.000000   \n",
       "7                 4231                 2             8         0.250000   \n",
       "8                 4560                 1             7         0.142857   \n",
       "9                 4726                 2             5         0.400000   \n",
       "10                4727                 1             1         1.000000   \n",
       "11                4728                 1             4         0.250000   \n",
       "12                4729                 2             9         0.222222   \n",
       "13                4730                 1             5         0.200000   \n",
       "14                5035                 1             2         0.500000   \n",
       "15                5224                 1             2         0.500000   \n",
       "16                5313                 0             3         0.000000   \n",
       "17                5729                 1             5         0.200000   \n",
       "18                7906                 1             3         0.333333   \n",
       "19                8481                 1             1         1.000000   \n",
       "20                8695                 3             6         0.500000   \n",
       "21                8994                 1             3         0.333333   \n",
       "22                9429                 0             1         0.000000   \n",
       "23                9835                 3             3         1.000000   \n",
       "24                9979                 2             7         0.285714   \n",
       "25               10193                 1             2         0.500000   \n",
       "26               10977                 2             3         0.666667   \n",
       "27               11223                 2             4         0.500000   \n",
       "28               11242                 2             7         0.285714   \n",
       "29               11585                 0             1         0.000000   \n",
       "30               11701                 2             3         0.666667   \n",
       "31               12005                 1             1         1.000000   \n",
       "32               13149                 2             4         0.500000   \n",
       "33               13154                 1             1         1.000000   \n",
       "34               13290                 1             1         1.000000   \n",
       "35               13901                 3             5         0.600000   \n",
       "36               16782                 1             1         1.000000   \n",
       "37               17920                 1             7         0.142857   \n",
       "38               18996                 2             7         0.285714   \n",
       "39               19701                 1            13         0.076923   \n",
       "40               19715                 2             5         0.400000   \n",
       "41               20669                 1             3         0.333333   \n",
       "42               20786                 3             6         0.500000   \n",
       "43               21180                 2             5         0.400000   \n",
       "44               22216                 3             5         0.600000   \n",
       "45               22751                 1             3         0.333333   \n",
       "46               22829                 1             1         1.000000   \n",
       "47               23255                 1             2         0.500000   \n",
       "48               24653                 2             4         0.500000   \n",
       "49               25491                 2             5         0.400000   \n",
       "\n",
       "    normalized_entropy  diversity  \n",
       "0                  NaN        NaN  \n",
       "1             0.434294   0.217147  \n",
       "2             0.434294   0.434294  \n",
       "3             1.169619   0.584809  \n",
       "4             1.763608   0.587869  \n",
       "5             0.434294   0.144765  \n",
       "6             1.169619   1.169619  \n",
       "7             1.165172   0.291293  \n",
       "8             0.434294   0.062042  \n",
       "9             1.112808   0.445123  \n",
       "10            0.434294   0.434294  \n",
       "11            0.434294   0.108574  \n",
       "12            1.128414   0.250759  \n",
       "13            0.434294   0.086859  \n",
       "14            0.434294   0.217147  \n",
       "15            0.434294   0.217147  \n",
       "16                 NaN        NaN  \n",
       "17            0.434294   0.086859  \n",
       "18            0.434294   0.144765  \n",
       "19            0.434294   0.434294  \n",
       "20            1.679661   0.839831  \n",
       "21            0.434294   0.144765  \n",
       "22                 NaN        NaN  \n",
       "23            1.780005   1.780005  \n",
       "24            1.145024   0.327150  \n",
       "25            0.434294   0.217147  \n",
       "26            1.145024   0.763349  \n",
       "27            1.112808   0.556404  \n",
       "28            1.145024   0.327150  \n",
       "29                 NaN        NaN  \n",
       "30            1.145024   0.763349  \n",
       "31            0.434294   0.434294  \n",
       "32            1.145024   0.572512  \n",
       "33            0.434294   0.434294  \n",
       "34            0.434294   0.434294  \n",
       "35            1.715581   1.029348  \n",
       "36            0.434294   0.434294  \n",
       "37            0.434294   0.062042  \n",
       "38            1.145024   0.327150  \n",
       "39            0.434294   0.033407  \n",
       "40            1.112808   0.445123  \n",
       "41            0.434294   0.144765  \n",
       "42            1.742131   0.871065  \n",
       "43            1.085911   0.434364  \n",
       "44            1.715581   1.029348  \n",
       "45            0.434294   0.144765  \n",
       "46            0.434294   0.434294  \n",
       "47            0.434294   0.217147  \n",
       "48            1.145024   0.572512  \n",
       "49            1.085911   0.434364  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diversity_final.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
